{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JCbarajas5/Text_Mining/blob/main/spanishBert_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu27gAv6UoTY"
      },
      "outputs": [],
      "source": [
        "!pip3 install pysentimiento\n",
        "!spacy download es_core_news_sm\n",
        "!nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2uKgUGXRnj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pysentimiento.preprocessing import preprocess_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAxLYCZUXVQw",
        "outputId": "c72ba53a-28a2-4f9c-e97c-3848741d0290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de tweets: 4594\n"
          ]
        }
      ],
      "source": [
        "with open('SENT-COVID.json') as file:\n",
        "    data = json.load(file)\n",
        "    \n",
        "pd.options.mode.chained_assignment = None                                         \n",
        "pd.set_option('display.max_colwidth',None)   \n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print('Numero de tweets: ' + str(len(df)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento"
      ],
      "metadata": {
        "id": "y1Al9zh-hTmp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAz-5PqQXZOJ"
      },
      "outputs": [],
      "source": [
        "def clean_tweet(text):\n",
        "  text = re.sub(r'[~^0-9]', '', text) #numeros\n",
        "  text = re.sub(\"\\\\s+\", ' ', text) ##Espacios blancos dobles\n",
        "  text = re.sub('\\n', ' ', text) ##Saltos de linea\n",
        "\n",
        "  pattern = r'([.])([A-Z#@¿])'\n",
        "  pattern2 = r'([-?])([a-zA-Z#@¿])'\n",
        "  pattern3 = r'([a-zA-Z])([#@¿(])'\n",
        "  pattern4 = r'([:!])([a-zA-Z#@¿])'\n",
        "  text = re.sub(pattern, r'\\1 \\2', text) # Separacion de punto seguido por una mayuscula\n",
        "  text = re.sub(pattern2, r'\\1 \\2', text)\n",
        "  text = re.sub(pattern3, r'\\1 \\2', text)\n",
        "  text = re.sub(pattern4, r'\\1 \\2', text)\n",
        "  return text \n",
        "\n",
        "\n",
        "def preprocess(text):  # Preprocesamiento de pysentimiento   \n",
        "  return preprocess_tweet(text, char_replace=True, normalize_laughter=True, shorten=3, \n",
        "                          emoji_wrapper='', user_token='usuario', url_token='url')  \n",
        "\n",
        "\n",
        "def normalize(text):\n",
        " pattern2 = r'([a-zA-Z])([.])'\n",
        " pattern3 = r'([.])([a-zA-Z])'\n",
        " text = re.sub(pattern2, r'\\1 \\2', text)\n",
        " text = re.sub(pattern3, r'\\1 \\2', text)\n",
        " \n",
        " text = \"\".join(u for u in text if u not in (\"?\",\"¿\", \".\", \";\", \":\", \"!\",\"¡\",'\"',\"%\",\"“\",\"”\",\"$\",\"&\",\"'\",\"\\\\\", \"(\",\")\",\n",
        "                                             \"*\",\"+\",\",\",\"/\",\"<\",\">\",\"=\",\"^\",\"•\",\"...\", \"ç\",\"π\",\"ⓘ\", \"-\", \"_\",\"#\",\"|\"))\n",
        " a,b = 'áéíóúÁÉÍÓÚ','aeiouAEIOU'\n",
        " trans = str.maketrans(a,b)     \n",
        " text = text.translate(trans) # Reemplazo de palabras acentuadas       \n",
        "\n",
        " pattern  = r'([a-z])([A-Z-])'\n",
        " text = re.sub(pattern, r'\\1 \\2', text)\n",
        "\n",
        " #text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        " text = text.lower()\n",
        " return text  \n",
        "\n",
        "\n",
        "def tokenize(text):    \n",
        "  text= text.split(sep = ' ')  # Tokenización por palabras individuales\n",
        "  text= [token for token in text if len(token) > 1]  # Eliminación de tokens con una longitud < 2\n",
        "  return(text) \n",
        "\n",
        "def labels(label):\n",
        "  if label == 'POSITIVO':\n",
        "    label=1\n",
        "  elif label == 'NEUTRO':\n",
        "     label=0\n",
        "  else:\n",
        "     label=-1\n",
        "  return(label) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l69gogzYXhOH"
      },
      "outputs": [],
      "source": [
        "df['clean_tweet'] = df['Tweet'].apply(clean_tweet) \n",
        "df['preprocess_tweet'] = df['clean_tweet'].apply(preprocess)\n",
        "df['normalized_tweet'] = df['preprocess_tweet'].apply(normalize)\n",
        "df['tokenized_tweet'] = df['normalized_tweet'].apply(tokenize)\n",
        "df['class'] = df['Label'].apply(labels)\n",
        "\n",
        "#df[['class', 'normalized_tweet', 'tokenized_tweet']].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatizacion"
      ],
      "metadata": {
        "id": "IhLbxfZ5iT1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "sp = spacy.load('es_core_news_sm')\n",
        "\n",
        "def lemmatization(text):\n",
        "    doc = sp(text)\n",
        "    return ' '.join([word.lemma_ for word in doc]) \n",
        "\n",
        "#stemmer = SnowballStemmer('spanish')\n",
        "#stemmed_spanish = [stemmer.stem(item) for item in spanish_words]\n",
        "\n",
        "df['lem_tweet'] = df['normalized_tweet'].apply(lemmatization)\n",
        "df['lemtokenized_tweet'] = df['lem_tweet'].apply(tokenize)"
      ],
      "metadata": {
        "id": "dCW35oaniI38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test Split"
      ],
      "metadata": {
        "id": "LSbU9Zighjwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7b_yDSEXrMN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['normalized_tweet']      #Tweets normalizados\n",
        "X2 = df['lem_tweet']            #Tweets lemmatizados\n",
        "X3 = df['tokenized_tweet']      #Normalizados y tokenizados\n",
        "X4 = df['lemtokenized_tweet']  #Lemmatizados y tokenizados \n",
        "y = df['class']                 #Etiquetas\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25 ,random_state=37)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoNdSsyfdAw5"
      },
      "source": [
        "## BERT (BETO)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "CVQle8j1j2RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESSTm_vurGXX"
      },
      "outputs": [],
      "source": [
        "!CUDA_LAUNCH_BLOCKING=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4oW4JVZ8gx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/pytorch_weights.tar.gz \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/vocab.txt \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/uncased_2M/config.json \n",
        "!wget https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/all/test_text.txt\n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K0Jqe19aBU3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from transformers import BertModel, BertTokenizer, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup, AutoConfig, AutoTokenizer\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from textwrap import wrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5i13dmKqPjf"
      },
      "source": [
        "## Inicializacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWn9VRZNchBu"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 37\n",
        "MAX_LEN = 480\n",
        "BATCH_SIZE = 16\n",
        "NCLASSES = 3\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE0a5_WgHu2R"
      },
      "source": [
        "## Tokenizacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWax422TrKi7"
      },
      "outputs": [],
      "source": [
        "#PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
        "#tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "#MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "#config = AutoConfig.from_pretrained(MODEL)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"pytorch/\", use_fast=True)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6wW-Bur6YWu"
      },
      "source": [
        "##Prueba con un solo sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTmhPdzWv078",
        "outputId": "71df675b-0cbd-4e39-8b9b-f577d22de11f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frase:  bueno pero es no le pidas demasiado mejor preguntenle de la fuerza moral de su patron\n",
            "Tokens:  ['bueno', 'pero', 'es', 'no', 'le', 'pidas', 'demasiado', 'mejor', 'pregunte', '##n', '##le', 'de', 'la', 'fuerza', 'moral', 'de', 'su', 'patr', '##on']\n",
            "Tokens numéricos:  [1491, 1195, 1028, 1054, 1165, 28903, 2668, 1544, 16216, 30959, 1080, 1009, 1032, 3193, 8003, 1009, 1069, 5102, 1022]\n"
          ]
        }
      ],
      "source": [
        "sample_txt = 'bueno pero es no le pidas demasiado mejor preguntenle de la fuerza moral de su patron' \n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print('Frase: ', sample_txt)\n",
        "print('Tokens: ', tokens)\n",
        "print('Tokens numéricos: ', token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j8KAne70utx",
        "outputId": "0a61385a-0cd5-456c-feca-2f766e94804c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Codificación:\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_txt,\n",
        "    max_length = 18,\n",
        "    truncation = True,\n",
        "    add_special_tokens = True,\n",
        "    return_token_type_ids = False,\n",
        "    pad_to_max_length = True,\n",
        "    return_attention_mask = True,\n",
        "    return_tensors = 'pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcitXdh2W8-",
        "outputId": "a9fa5dde-4ed0-4c71-9a29-066cd3e979f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'bueno', 'pero', 'es', 'no', 'le', 'pidas', 'demasiado', 'mejor', 'pregunte', '##n', '##le', 'de', 'la', 'fuerza', 'moral', 'de', '[SEP]']\n",
            "tensor([    4,  1491,  1195,  1028,  1054,  1165, 28903,  2668,  1544, 16216,\n",
            "        30959,  1080,  1009,  1032,  3193,  8003,  1009,     5])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(encoding['input_ids'][0]))\n",
        "print(encoding['input_ids'][0])\n",
        "print(encoding['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYsTFKxR6flW"
      },
      "source": [
        "## CREACIÓN DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSaCQbDj4wbG"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "\n",
        "  def __init__(self,reviews,labels,tokenizer,max_len):\n",
        "    self.reviews = reviews\n",
        "    self.labels = labels\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.reviews)\n",
        "    \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    label = self.labels[item]\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        review,\n",
        "        max_length = self.max_len,\n",
        "        truncation = True,\n",
        "        add_special_tokens = True,\n",
        "        return_token_type_ids = False,\n",
        "        pad_to_max_length = True,\n",
        "        return_attention_mask = True,\n",
        "        return_tensors = 'pt'\n",
        "        )\n",
        "    \n",
        "\n",
        "    return {\n",
        "          'review': review,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'label': torch.tensor(label, dtype=torch.long)\n",
        "      } "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly7x2h4wA0hz"
      },
      "source": [
        "## Data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCNgmS7oAyyR"
      },
      "outputs": [],
      "source": [
        "def data_loader(df, tokenizer, max_len, batch_size):\n",
        "  dataset = IMDBDataset(\n",
        "      reviews = df.review.to_numpy(),\n",
        "      labels = df.label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = MAX_LEN\n",
        "  )\n",
        "\n",
        "  return DataLoader(dataset, batch_size = BATCH_SIZE, num_workers = 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIyk44tfEBJ7"
      },
      "outputs": [],
      "source": [
        "df1 = df[['normalized_tweet', 'class']]\n",
        "df1.columns = ['review', 'label']\n",
        "df_train, df_test = train_test_split(df1, test_size = 0.25, random_state=RANDOM_SEED)\n",
        "\n",
        "train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQBqsIwHfmk"
      },
      "source": [
        "##Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUtCD9ybHe78"
      },
      "outputs": [],
      "source": [
        "class BERTSentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(BERTSentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('pytorch/')\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, cls_output = self.bert(\n",
        "        input_ids = input_ids,\n",
        "        attention_mask = attention_mask,\n",
        "        return_dict=False\n",
        "    )\n",
        "\n",
        "    drop_output = self.drop(cls_output)\n",
        "    output = self.linear(drop_output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWFzXibbMiuF",
        "outputId": "eabac8af-c282-4033-8aad-88a1b4a1e826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model = BERTSentimentClassifier(NCLASSES)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEewkTZnON37"
      },
      "source": [
        "## ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk3oAPmcOKNd",
        "outputId": "041a1ef3-8c09-42b6-e003-d73475bb80c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 5\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps = 0,\n",
        "    num_training_steps = total_steps\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK3HkpRtO-WP"
      },
      "outputs": [],
      "source": [
        "# Iteración entrenamiento\n",
        "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  for batch in data_loader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, labels)\n",
        "    correct_predictions += torch.sum(preds == labels)\n",
        "    losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses)\n",
        "\n",
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['label'].to(device)\n",
        "      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      correct_predictions += torch.sum(preds == labels)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double()/n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_9XZ-xYRZFo"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n",
        "  print('------------------')\n",
        "  train_acc, train_loss = train_model(\n",
        "      model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n",
        "  )\n",
        "  test_acc, test_loss = eval_model(\n",
        "      model, test_data_loader, loss_fn, device, len(df_test)\n",
        "  )\n",
        "  print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n",
        "  print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n",
        "  print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgxBDGyITwGj"
      },
      "source": [
        "##Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgx8VouTTulG"
      },
      "outputs": [],
      "source": [
        "def classifySentiment(review_text):\n",
        "  encoding_review = tokenizer.encode_plus(\n",
        "      review_text,\n",
        "      max_length = MAX_LEN,\n",
        "      truncation = True,\n",
        "      add_special_tokens = True,\n",
        "      return_token_type_ids = False,\n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt'\n",
        "      )\n",
        "  \n",
        "  input_ids = encoding_review['input_ids'].to(device)\n",
        "  attention_mask = encoding_review['attention_mask'].to(device)\n",
        "  output = model(input_ids, attention_mask)\n",
        "  _, prediction = torch.max(output, dim=1)\n",
        "  print(\"\\n\".join(wrap(review_text)))\n",
        "  if prediction == False:\n",
        "    print('Sentimiento predicho: * * * * *')\n",
        "  else:\n",
        "    print('Sentimiento predicho: *')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0ysQPCTUM3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aceb9b2-c72d-4f8f-cf7f-64a09c7d61b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mal\n",
            "Sentimiento predicho: * * * * *\n"
          ]
        }
      ],
      "source": [
        "review_text = \"mal\"\n",
        "\n",
        "classifySentiment(review_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y1Al9zh-hTmp",
        "IhLbxfZ5iT1b",
        "r6wW-Bur6YWu"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMskIBea15arQ+ZAx0XuXJG",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}