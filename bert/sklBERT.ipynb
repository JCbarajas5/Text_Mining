{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from pysentimiento.preprocessing import preprocess_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus SENT-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de tweets: 4594\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>-@dulcema201 @BronstonRaqsa02 Protocolo de COVID !!!!</td>\n",
       "      <td>1401047081121353728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>-#COVID19 #QuedateEnCasa en Morelia Centro</td>\n",
       "      <td>1258159310162595843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVO</td>\n",
       "      <td>-M√©xico va en en aumento con el #Covid_19. Tal vez no tengamos la estabilidad de Europa o estados unidos. Para mantener 120 d√≠as en paro total. Pero podemos ser precavidos al usar la #SanaDistancia</td>\n",
       "      <td>1272748988626862082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>-@sororavirus Creo en todo y nada. üíú</td>\n",
       "      <td>1349385638722883585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>-@GobiernoMX hab√≠a prometido 389</td>\n",
       "      <td>1360615587114844161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label  \\\n",
       "0    NEUTRO   \n",
       "1    NEUTRO   \n",
       "2  POSITIVO   \n",
       "3    NEUTRO   \n",
       "4  NEGATIVO   \n",
       "\n",
       "                                                                                                                                                                                                   Tweet  \\\n",
       "0                                                                                                                                                  -@dulcema201 @BronstonRaqsa02 Protocolo de COVID !!!!   \n",
       "1                                                                                                                                                             -#COVID19 #QuedateEnCasa en Morelia Centro   \n",
       "2  -M√©xico va en en aumento con el #Covid_19. Tal vez no tengamos la estabilidad de Europa o estados unidos. Para mantener 120 d√≠as en paro total. Pero podemos ser precavidos al usar la #SanaDistancia   \n",
       "3                                                                                                                                                                   -@sororavirus Creo en todo y nada. üíú   \n",
       "4                                                                                                                                                                       -@GobiernoMX hab√≠a prometido 389   \n",
       "\n",
       "                    id  \n",
       "0  1401047081121353728  \n",
       "1  1258159310162595843  \n",
       "2  1272748988626862082  \n",
       "3  1349385638722883585  \n",
       "4  1360615587114844161  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/SENT-COVID.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "pd.options.mode.chained_assignment = None                                         \n",
    "pd.set_option('display.max_colwidth',None)   \n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print('Numero de tweets: ' + str(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "  text = re.sub(r'[~^0-9]', '', text) #numeros\n",
    "  text = re.sub(\"\\\\s+\", ' ', text) ##Espacios blancos dobles\n",
    "  text = re.sub('\\n', ' ', text) ##Saltos de linea\n",
    "\n",
    "  pattern = r'([.])([A-Z#@¬ø])'\n",
    "  pattern2 = r'([-])([a-zA-Z#@¬ø])'\n",
    "  pattern3 = r'([a-zA-Z])([#@¬ø])'\n",
    "  pattern4 = r'([:!])([a-zA-Z#@¬ø])'\n",
    "  text = re.sub(pattern, r'\\1 \\2', text) # Separacion de punto seguido por una mayuscula\n",
    "  text = re.sub(pattern2, r'\\1 \\2', text)\n",
    "  text = re.sub(pattern3, r'\\1 \\2', text)\n",
    "  text = re.sub(pattern4, r'\\1 \\2', text)\n",
    "  return text \n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "  return preprocess_tweet(text, shorten=2, emoji_wrapper='', user_token='')  # Preprocesamiento de pysentimiento   \n",
    "\n",
    "\n",
    "def normalize(text):\n",
    " text = \"\".join(u for u in text if u not in (\"?\",\"¬ø\", \".\", \";\", \":\", \"!\",\"¬°\",'\"',\"%\",\"‚Äú\",\"‚Äù\",\"$\",\"&\",\"'\",\"\\\\\", \"(\",\")\",\n",
    "                                             \"*\",\"+\",\",\",\"/\",\"<\",\">\",\"=\",\"^\",\"‚Ä¢\",\"...\", \"√ß\",\"œÄ\",\"‚ìò\", \"-\", \"_\",\"#\",\"|\"))\n",
    " a,b = '√°√©√≠√≥√∫√Å√â√ç√ì√ö','aeiouAEIOU'\n",
    " trans = str.maketrans(a,b)     \n",
    " text = text.translate(trans) # Reemplazo de palabras acentuadas       \n",
    "\n",
    " pattern = r'([a-z])([A-Z-])'\n",
    " text = re.sub(pattern, r'\\1 \\2', text)\n",
    " #text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    " text = text.lower()\n",
    " return text  \n",
    "\n",
    "\n",
    "def tokenize(text):    \n",
    "  text= text.split(sep = ' ')  # Tokenizaci√≥n por palabras individuales\n",
    "  text= [token for token in text if len(token) > 1]  # Eliminaci√≥n de tokens con una longitud < 2\n",
    "  return(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>norm_tweet</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-#SNTEsalud ‚öïÔ∏è‚ö†Ô∏è ALERTA ‚ö†Ô∏è M√©xico vive alto contagio #coronavirus #COVID19 ü¶† Ante s√≠ntomas no te automediques üíäüö´ ni tomes f√°rmacos que prometen curar el #COVID üíä‚ùå y llama al üì≤ 800 00 44 800üì±#QuedateEnCasa#FelizViernes #22DeMayo M√©xico #Secci√≥n35 #FelizFinde #RT https://t.co/wNIfhlJflO</td>\n",
       "      <td>sntesalud   simbolo de medicina    advertencia   alerta   advertencia   mexico vive alto contagio coronavirus covid   microbio   ante sintomas no te automediques   pildora    prohibido   ni tomes farmacos que prometen curar el covid   pildora    marca de cruz   y llama al   movil con una flecha     telefono movil  quedate en casa feliz viernes de mayo mexico seccion feliz finde rt url</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-@LOVREGA @FelipeCalderon Pues como foca aplaudidora te queda bien el papel</td>\n",
       "      <td>pues como foca aplaudidora te queda bien el papel</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-\"No habr√° un \"\"regreso a la normalidad\"\" en el mundo tras la pandemia de covid-19 https://t.co/bRO7munVtA v√≠a @UniNoticias @dadams7308\"</td>\n",
       "      <td>no habra un regreso a la normalidad en el mundo tras la pandemia de covid url via</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-Ya le hizo da√±o la vacuna.Cuando yo viv√≠a en Alemania todo era paz y tranquilidad</td>\n",
       "      <td>ya le hizo da√±o la vacuna cuando yo vivia en alemania todo era paz y tranquilidad</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-Marcarle a mi preciosita en momento de crisis. ü•∫ü•∫ü•∫</td>\n",
       "      <td>marcarle a mi preciosita en momento de crisis   cara de por favor    cara de por favor</td>\n",
       "      <td>POSITIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-@albertoviruete @nenulo Esos hermanos Negrete una bola de vividores</td>\n",
       "      <td>esos hermanos negrete una bola de vividores</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-\"@rayados est√° en crisis- la Crisis https://t.co/aAYKlKFfFS\"</td>\n",
       "      <td>esta en crisis la crisis url</td>\n",
       "      <td>NEUTRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-Uff üò• ü¶†ü¶†ü¶†#QuedateEnCasaüè° #Coahuila #Mexico üò¢</td>\n",
       "      <td>uff   cara triste pero aliviada     microbio    microbio  quedate en casa  casa con jardin   coahuila mexico   cara llorando</td>\n",
       "      <td>NEGATIVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-No s√© qu√© tanto maman con #LadyVacuna</td>\n",
       "      <td>no se que tanto maman con lady vacuna</td>\n",
       "      <td>NEUTRO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-\"@19991007 @soylajefita Ojo</td>\n",
       "      <td>ojo</td>\n",
       "      <td>NEUTRO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             Tweet  \\\n",
       "95   -#SNTEsalud ‚öïÔ∏è‚ö†Ô∏è ALERTA ‚ö†Ô∏è M√©xico vive alto contagio #coronavirus #COVID19 ü¶† Ante s√≠ntomas no te automediques üíäüö´ ni tomes f√°rmacos que prometen curar el #COVID üíä‚ùå y llama al üì≤ 800 00 44 800üì±#QuedateEnCasa#FelizViernes #22DeMayo M√©xico #Secci√≥n35 #FelizFinde #RT https://t.co/wNIfhlJflO   \n",
       "96                                                                                                                                                                                                                     -@LOVREGA @FelipeCalderon Pues como foca aplaudidora te queda bien el papel   \n",
       "97                                                                                                                                                        -\"No habr√° un \"\"regreso a la normalidad\"\" en el mundo tras la pandemia de covid-19 https://t.co/bRO7munVtA v√≠a @UniNoticias @dadams7308\"   \n",
       "98                                                                                                                                                                                                              -Ya le hizo da√±o la vacuna.Cuando yo viv√≠a en Alemania todo era paz y tranquilidad   \n",
       "99                                                                                                                                                                                                                                             -Marcarle a mi preciosita en momento de crisis. ü•∫ü•∫ü•∫   \n",
       "100                                                                                                                                                                                                                           -@albertoviruete @nenulo Esos hermanos Negrete una bola de vividores   \n",
       "101                                                                                                                                                                                                                                  -\"@rayados est√° en crisis- la Crisis https://t.co/aAYKlKFfFS\"   \n",
       "102                                                                                                                                                                                                                                                  -Uff üò• ü¶†ü¶†ü¶†#QuedateEnCasaüè° #Coahuila #Mexico üò¢   \n",
       "103                                                                                                                                                                                                                                                         -No s√© qu√© tanto maman con #LadyVacuna   \n",
       "104                                                                                                                                                                                                                                                                   -\"@19991007 @soylajefita Ojo   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                               norm_tweet  \\\n",
       "95    sntesalud   simbolo de medicina    advertencia   alerta   advertencia   mexico vive alto contagio coronavirus covid   microbio   ante sintomas no te automediques   pildora    prohibido   ni tomes farmacos que prometen curar el covid   pildora    marca de cruz   y llama al   movil con una flecha     telefono movil  quedate en casa feliz viernes de mayo mexico seccion feliz finde rt url   \n",
       "96                                                                                                                                                                                                                                                                                                                                                      pues como foca aplaudidora te queda bien el papel   \n",
       "97                                                                                                                                                                                                                                                                                                                    no habra un regreso a la normalidad en el mundo tras la pandemia de covid url via     \n",
       "98                                                                                                                                                                                                                                                                                                                      ya le hizo da√±o la vacuna cuando yo vivia en alemania todo era paz y tranquilidad   \n",
       "99                                                                                                                                                                                                                                                                                                                 marcarle a mi preciosita en momento de crisis   cara de por favor    cara de por favor   \n",
       "100                                                                                                                                                                                                                                                                                                                                                           esos hermanos negrete una bola de vividores   \n",
       "101                                                                                                                                                                                                                                                                                                                                                                          esta en crisis la crisis url   \n",
       "102                                                                                                                                                                                                                                                                          uff   cara triste pero aliviada     microbio    microbio  quedate en casa  casa con jardin   coahuila mexico   cara llorando   \n",
       "103                                                                                                                                                                                                                                                                                                                                                                 no se que tanto maman con lady vacuna   \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                   ojo   \n",
       "\n",
       "        Label  \n",
       "95   NEGATIVO  \n",
       "96   NEGATIVO  \n",
       "97   NEGATIVO  \n",
       "98   NEGATIVO  \n",
       "99   POSITIVO  \n",
       "100  NEGATIVO  \n",
       "101    NEUTRO  \n",
       "102  NEGATIVO  \n",
       "103    NEUTRO  \n",
       "104    NEUTRO  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweet'] = df['Tweet'].apply(clean_tweet) \n",
    "df['preprocess_tweet'] = df['clean_tweet'].apply(preprocess)\n",
    "df['norm_tweet'] = df['preprocess_tweet'].apply(normalize)\n",
    "df['tokenized_tweet'] = df['norm_tweet'].apply(tokenize)\n",
    "\n",
    "df[['Tweet','norm_tweet','Label']][95:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def lemmatization(text):\n",
    "    doc = sp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc]) \n",
    "\n",
    "#stemmer = SnowballStemmer('spanish')\n",
    "#stemmed_spanish = [stemmer.stem(item) for item in spanish_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>norm_tweet</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>lemtokenized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>esos hermanos negrete una bola de vividores</td>\n",
       "      <td>ese hermano negretir uno bola de vividor</td>\n",
       "      <td>[esos, hermanos, negrete, una, bola, de, vividores]</td>\n",
       "      <td>[ese, hermano, negretir, uno, bola, de, vividor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>esta en crisis la crisis url</td>\n",
       "      <td>este en crisis el crisis url</td>\n",
       "      <td>[esta, en, crisis, la, crisis, url]</td>\n",
       "      <td>[este, en, crisis, el, crisis, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>uff   cara triste pero aliviada     microbio    microbio  quedate en casa  casa con jardin   coahuila mexico   cara llorando</td>\n",
       "      <td>uff    cara triste pero aliviado      microbio     microbio   quedate en casa   casa con jardin    coahuila mexico    caro llorar</td>\n",
       "      <td>[uff, cara, triste, pero, aliviada, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, cara, llorando]</td>\n",
       "      <td>[uff, cara, triste, pero, aliviado, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, caro, llorar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>no se que tanto maman con lady vacuna</td>\n",
       "      <td>no √©l que tanto mamar con lady vacuna</td>\n",
       "      <td>[no, se, que, tanto, maman, con, lady, vacuna]</td>\n",
       "      <td>[no, √©l, que, tanto, mamar, con, lady, vacuna]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>ojo</td>\n",
       "      <td>ojo</td>\n",
       "      <td>[ojo]</td>\n",
       "      <td>[ojo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>hoy en el programa  versiones raras</td>\n",
       "      <td>hoy en el programa   versi√≥n rara</td>\n",
       "      <td>[hoy, en, el, programa, versiones, raras]</td>\n",
       "      <td>[hoy, en, el, programa, versi√≥n, rara]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>hoy participamos nuevamente para el programa de opinion yo creo</td>\n",
       "      <td>hoy participamos nuevamente para el programa de opinion yo creer</td>\n",
       "      <td>[hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creo]</td>\n",
       "      <td>[hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>flexibilidad</td>\n",
       "      <td>flexibilidad</td>\n",
       "      <td>[flexibilidad]</td>\n",
       "      <td>[flexibilidad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>POSITIVO</td>\n",
       "      <td>dios los cuide y les conceda una pronta recuperacion y que cuide y bendiga a aquellos que aun no han presentado sintomas de contagio</td>\n",
       "      <td>dio √©l cuide y √©l concedar uno pronto recuperacion y que cuidir y bendiga a aquel que aun no haber presentar sintoma de contagio</td>\n",
       "      <td>[dios, los, cuide, les, conceda, una, pronta, recuperacion, que, cuide, bendiga, aquellos, que, aun, no, han, presentado, sintomas, de, contagio]</td>\n",
       "      <td>[dio, √©l, cuide, √©l, concedar, uno, pronto, recuperacion, que, cuidir, bendiga, aquel, que, aun, no, haber, presentar, sintoma, de, contagio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>y las sanciones para  y  por el terrible manejo de la crisis que</td>\n",
       "      <td>y el sanci√≥n para   y   por el terrible manejo de el crisis que</td>\n",
       "      <td>[las, sanciones, para, por, el, terrible, manejo, de, la, crisis, que]</td>\n",
       "      <td>[el, sanci√≥n, para, por, el, terrible, manejo, de, el, crisis, que]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label  \\\n",
       "100  NEGATIVO   \n",
       "101    NEUTRO   \n",
       "102  NEGATIVO   \n",
       "103    NEUTRO   \n",
       "104    NEUTRO   \n",
       "105    NEUTRO   \n",
       "106    NEUTRO   \n",
       "107    NEUTRO   \n",
       "108  POSITIVO   \n",
       "109  NEGATIVO   \n",
       "\n",
       "                                                                                                                                 norm_tweet  \\\n",
       "100                                                                                             esos hermanos negrete una bola de vividores   \n",
       "101                                                                                                            esta en crisis la crisis url   \n",
       "102            uff   cara triste pero aliviada     microbio    microbio  quedate en casa  casa con jardin   coahuila mexico   cara llorando   \n",
       "103                                                                                                   no se que tanto maman con lady vacuna   \n",
       "104                                                                                                                                     ojo   \n",
       "105                                                                                                     hoy en el programa  versiones raras   \n",
       "106                                                                         hoy participamos nuevamente para el programa de opinion yo creo   \n",
       "107                                                                                                                            flexibilidad   \n",
       "108    dios los cuide y les conceda una pronta recuperacion y que cuide y bendiga a aquellos que aun no han presentado sintomas de contagio   \n",
       "109                                                                        y las sanciones para  y  por el terrible manejo de la crisis que   \n",
       "\n",
       "                                                                                                                               lem_tweet  \\\n",
       "100                                                                                             ese hermano negretir uno bola de vividor   \n",
       "101                                                                                                         este en crisis el crisis url   \n",
       "102    uff    cara triste pero aliviado      microbio     microbio   quedate en casa   casa con jardin    coahuila mexico    caro llorar   \n",
       "103                                                                                                no √©l que tanto mamar con lady vacuna   \n",
       "104                                                                                                                                  ojo   \n",
       "105                                                                                                    hoy en el programa   versi√≥n rara   \n",
       "106                                                                     hoy participamos nuevamente para el programa de opinion yo creer   \n",
       "107                                                                                                                         flexibilidad   \n",
       "108     dio √©l cuide y √©l concedar uno pronto recuperacion y que cuidir y bendiga a aquel que aun no haber presentar sintoma de contagio   \n",
       "109                                                                      y el sanci√≥n para   y   por el terrible manejo de el crisis que   \n",
       "\n",
       "                                                                                                                                       tokenized_tweet  \\\n",
       "100                                                                                                [esos, hermanos, negrete, una, bola, de, vividores]   \n",
       "101                                                                                                                [esta, en, crisis, la, crisis, url]   \n",
       "102                    [uff, cara, triste, pero, aliviada, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, cara, llorando]   \n",
       "103                                                                                                     [no, se, que, tanto, maman, con, lady, vacuna]   \n",
       "104                                                                                                                                              [ojo]   \n",
       "105                                                                                                          [hoy, en, el, programa, versiones, raras]   \n",
       "106                                                                         [hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creo]   \n",
       "107                                                                                                                                     [flexibilidad]   \n",
       "108  [dios, los, cuide, les, conceda, una, pronta, recuperacion, que, cuide, bendiga, aquellos, que, aun, no, han, presentado, sintomas, de, contagio]   \n",
       "109                                                                             [las, sanciones, para, por, el, terrible, manejo, de, la, crisis, que]   \n",
       "\n",
       "                                                                                                                                lemtokenized_tweet  \n",
       "100                                                                                               [ese, hermano, negretir, uno, bola, de, vividor]  \n",
       "101                                                                                                            [este, en, crisis, el, crisis, url]  \n",
       "102                  [uff, cara, triste, pero, aliviado, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, caro, llorar]  \n",
       "103                                                                                                 [no, √©l, que, tanto, mamar, con, lady, vacuna]  \n",
       "104                                                                                                                                          [ojo]  \n",
       "105                                                                                                         [hoy, en, el, programa, versi√≥n, rara]  \n",
       "106                                                                    [hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creer]  \n",
       "107                                                                                                                                 [flexibilidad]  \n",
       "108  [dio, √©l, cuide, √©l, concedar, uno, pronto, recuperacion, que, cuidir, bendiga, aquel, que, aun, no, haber, presentar, sintoma, de, contagio]  \n",
       "109                                                                            [el, sanci√≥n, para, por, el, terrible, manejo, de, el, crisis, que]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lem_tweet'] = df['norm_tweet'].apply(lemmatization)\n",
    "df['lemtokenized_tweet'] = df['lem_tweet'].apply(tokenize)\n",
    "df[['Label', 'norm_tweet','lem_tweet','tokenized_tweet','lemtokenized_tweet']][100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'm√°s', 'pero', 'sus', 'le', 'ya', 'o', 'este', 's√≠', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'tambi√©n', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'm√≠', 'antes', 'algunos', 'qu√©', 'unos', 'yo', 'otro', 'otras', 'otra', '√©l', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 't√∫', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'm√≠o', 'm√≠a', 'm√≠os', 'm√≠as', 'tuyo']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>lemtokenized_tweet</th>\n",
       "      <th>lemtoksw_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>[ese, hermano, negretir, uno, bola, de, vividor]</td>\n",
       "      <td>[hermano, negretir, bola, vividor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[este, en, crisis, el, crisis, url]</td>\n",
       "      <td>[crisis, crisis, url]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>[uff, cara, triste, pero, aliviado, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, caro, llorar]</td>\n",
       "      <td>[uff, cara, triste, aliviado, microbio, microbio, quedate, casa, casa, jardin, coahuila, mexico, caro, llorar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[no, √©l, que, tanto, mamar, con, lady, vacuna]</td>\n",
       "      <td>[mamar, lady, vacuna]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[ojo]</td>\n",
       "      <td>[ojo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[hoy, en, el, programa, versi√≥n, rara]</td>\n",
       "      <td>[hoy, programa, versi√≥n, rara]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creer]</td>\n",
       "      <td>[hoy, participamos, nuevamente, programa, opinion, creer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>NEUTRO</td>\n",
       "      <td>[flexibilidad]</td>\n",
       "      <td>[flexibilidad]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>POSITIVO</td>\n",
       "      <td>[dio, √©l, cuide, √©l, concedar, uno, pronto, recuperacion, que, cuidir, bendiga, aquel, que, aun, no, haber, presentar, sintoma, de, contagio]</td>\n",
       "      <td>[dio, cuide, concedar, pronto, recuperacion, cuidir, bendiga, aquel, aun, haber, presentar, sintoma, contagio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>NEGATIVO</td>\n",
       "      <td>[el, sanci√≥n, para, por, el, terrible, manejo, de, el, crisis, que]</td>\n",
       "      <td>[sanci√≥n, terrible, manejo, crisis]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Label  \\\n",
       "100  NEGATIVO   \n",
       "101    NEUTRO   \n",
       "102  NEGATIVO   \n",
       "103    NEUTRO   \n",
       "104    NEUTRO   \n",
       "105    NEUTRO   \n",
       "106    NEUTRO   \n",
       "107    NEUTRO   \n",
       "108  POSITIVO   \n",
       "109  NEGATIVO   \n",
       "\n",
       "                                                                                                                                lemtokenized_tweet  \\\n",
       "100                                                                                               [ese, hermano, negretir, uno, bola, de, vividor]   \n",
       "101                                                                                                            [este, en, crisis, el, crisis, url]   \n",
       "102                  [uff, cara, triste, pero, aliviado, microbio, microbio, quedate, en, casa, casa, con, jardin, coahuila, mexico, caro, llorar]   \n",
       "103                                                                                                 [no, √©l, que, tanto, mamar, con, lady, vacuna]   \n",
       "104                                                                                                                                          [ojo]   \n",
       "105                                                                                                         [hoy, en, el, programa, versi√≥n, rara]   \n",
       "106                                                                    [hoy, participamos, nuevamente, para, el, programa, de, opinion, yo, creer]   \n",
       "107                                                                                                                                 [flexibilidad]   \n",
       "108  [dio, √©l, cuide, √©l, concedar, uno, pronto, recuperacion, que, cuidir, bendiga, aquel, que, aun, no, haber, presentar, sintoma, de, contagio]   \n",
       "109                                                                            [el, sanci√≥n, para, por, el, terrible, manejo, de, el, crisis, que]   \n",
       "\n",
       "                                                                                                     lemtoksw_tweet  \n",
       "100                                                                              [hermano, negretir, bola, vividor]  \n",
       "101                                                                                           [crisis, crisis, url]  \n",
       "102  [uff, cara, triste, aliviado, microbio, microbio, quedate, casa, casa, jardin, coahuila, mexico, caro, llorar]  \n",
       "103                                                                                           [mamar, lady, vacuna]  \n",
       "104                                                                                                           [ojo]  \n",
       "105                                                                                  [hoy, programa, versi√≥n, rara]  \n",
       "106                                                       [hoy, participamos, nuevamente, programa, opinion, creer]  \n",
       "107                                                                                                  [flexibilidad]  \n",
       "108  [dio, cuide, concedar, pronto, recuperacion, cuidir, bendiga, aquel, aun, haber, presentar, sintoma, contagio]  \n",
       "109                                                                             [sanci√≥n, terrible, manejo, crisis]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Obtenci√≥n de listado de stopwords del espa√±ol\n",
    "stop_words_esp = list(stopwords.words('spanish'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [w for w in text if not w in stop_words_esp]\n",
    "    return text\n",
    "\n",
    "df['normsw_tweet'] = df['norm_tweet'].apply(remove_stopwords)\n",
    "df['lemsw_tweet'] = df['lem_tweet'].apply(remove_stopwords)\n",
    "df['normtoksw_tweet'] = df['tokenized_tweet'].apply(remove_stopwords)\n",
    "df['lemtoksw_tweet'] = df['lemtokenized_tweet'].apply(remove_stopwords)\n",
    "\n",
    "print(stop_words_esp[:100])\n",
    "df[['Label', 'lemtokenized_tweet', 'lemtoksw_tweet']][100:110]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1 = df['norm_tweet']          #Tweets normalizados\n",
    "X2 = df['lem_tweet']           #Tweets lemmatizados\n",
    "X3 = df['tokenized_tweet']     #Normalizados y tokenizados\n",
    "X4 = df['lemtokenized_tweet']  #Lemmatizados y tokenizados \n",
    "X5 = df['normtoksw_tweet']     #Normalizados, tokenizados y sin stopwords\n",
    "X6 = df['lemtoksw_tweet']      #Lemmatizados, tokenizados y sin stopwords \n",
    "y = df['Label']                #Etiquetas\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.25 ,random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NEGATIVO': 33.64296081277213, 'NEUTRO': 44.93468795355588, 'POSITIVO': 21.42235123367199}\n",
      "{'NEGATIVO': 34.89991296779809, 'NEUTRO': 44.38642297650131, 'POSITIVO': 20.713664055700608}\n"
     ]
    }
   ],
   "source": [
    "value, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))\n",
    "value, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de features: 2807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;m√°s&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;s√≠&#x27;,\n",
       "                            &#x27;porque&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;m√°s&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;s√≠&#x27;,\n",
       "                            &#x27;porque&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
       "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
       "                            'no', 'una', 'su', 'al', 'lo', 'como', 'm√°s',\n",
       "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 's√≠',\n",
       "                            'porque', ...])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf= TfidfVectorizer(min_df=3, ngram_range=(1,2), stop_words = stop_words_esp).fit(X_train)\n",
    "                        \n",
    "print('Numero de features: ' +str(len(tfidf.get_feature_names_out())))\n",
    "tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3445x2807 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30268 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "X_train_tfidf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de features: 2807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;m√°s&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;s√≠&#x27;,\n",
       "                            &#x27;porque&#x27;, ...])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=[&#x27;de&#x27;, &#x27;la&#x27;, &#x27;que&#x27;, &#x27;el&#x27;, &#x27;en&#x27;, &#x27;y&#x27;, &#x27;a&#x27;, &#x27;los&#x27;,\n",
       "                            &#x27;del&#x27;, &#x27;se&#x27;, &#x27;las&#x27;, &#x27;por&#x27;, &#x27;un&#x27;, &#x27;para&#x27;, &#x27;con&#x27;,\n",
       "                            &#x27;no&#x27;, &#x27;una&#x27;, &#x27;su&#x27;, &#x27;al&#x27;, &#x27;lo&#x27;, &#x27;como&#x27;, &#x27;m√°s&#x27;,\n",
       "                            &#x27;pero&#x27;, &#x27;sus&#x27;, &#x27;le&#x27;, &#x27;ya&#x27;, &#x27;o&#x27;, &#x27;este&#x27;, &#x27;s√≠&#x27;,\n",
       "                            &#x27;porque&#x27;, ...])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(min_df=3, ngram_range=(1, 2),\n",
       "                stop_words=['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los',\n",
       "                            'del', 'se', 'las', 'por', 'un', 'para', 'con',\n",
       "                            'no', 'una', 'su', 'al', 'lo', 'como', 'm√°s',\n",
       "                            'pero', 'sus', 'le', 'ya', 'o', 'este', 's√≠',\n",
       "                            'porque', ...])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvect = CountVectorizer(min_df=3, stop_words = stop_words_esp, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "print('Numero de features: ' +str(len(countvect.get_feature_names_out())))\n",
    "countvect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3445x2807 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 30268 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv = countvect.transform(X_train)\n",
    "X_test_cv  = countvect.transform(X_test)\n",
    "X_train_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models import FastText\n",
    "\n",
    "wordvectors_file = 'data/wiki.es.vec'\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file, limit=600000)\n",
    "\n",
    "#embedding=200\n",
    "#w2v = Word2Vec(X6, min_count=3, vector_size=embedding, window=5, sg=1 )\n",
    "#w2v.train(X6, total_examples= len(df['lem_tweet']), epochs=20)\n",
    "\n",
    "#wordvectors.most_similar('pozole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4594, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += wordvectors[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "wordvec_arrays = np.zeros((len(X6), 300)) \n",
    "for i in range(len(X6)):\n",
    "    wordvec_arrays[i,:] = word_vector(X6[i], 300)\n",
    "    \n",
    "X_w2v = pd.DataFrame(wordvec_arrays)\n",
    "X_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N√∫mero de tokens creados: 2807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['abajo', 'abajo flechaber', 'abajo tono', ..., 'zeneca', 'zona',\n",
       "       'zoom'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulario CountVectorizer:\n",
    "print(f\" N√∫mero de tokens creados: {len(countvect.get_feature_names_out())}\")\n",
    "countvect.get_feature_names_out()\n",
    "\n",
    "# vocabuilario TFIDF:\n",
    "#print(f\" N√∫mero de tokens creados: {len(tfidf.get_feature_names_out())}\")\n",
    "#tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 231508/231508 [00:00<00:00, 13787537.88B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440473133/440473133 [00:59<00:00, 7379588.20B/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 433/433 [00:00<00:00, 147785.31B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 3101, validation data size: 344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Training  :   0%|          | 0/97 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bert_sklearn/model/pytorch_pretrained/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /Users/distiller/project/pytorch/torch/csrc/utils/python_arg_parser.cpp:1055.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  :   5%|‚ñå         | 5/97 [11:29<2:27:24, 96.13s/it, loss=1.11]  "
     ]
    }
   ],
   "source": [
    "from bert_sklearn import BertClassifier\n",
    "from bert_sklearn import load_model\n",
    "\n",
    "model = BertClassifier()         # text/text pair classification\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)\n",
    "scr = model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
